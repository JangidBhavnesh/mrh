#if defined(_GPU_SYCL) || defined(_GPU_SYCL_CUDA)

#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <iostream>
#include <cassert>

#include "pm.h"

#define _TRANSPOSE_BLOCK_SIZE 16
#define _TRANSPOSE_NUM_ROWS 16

//#define _TILE(A,B)  (A + B - 1) / B
#define _TILE(A,B) ((A + B - 1) / B) * B

#ifdef _SINGLE_PRECISION
  typedef float real_t;
#else
  typedef double real_t;
#endif

using namespace PM_NS;

class PM * pm_ = nullptr;

class Kernel_Copy_Naive;
class Kernel_Transpose_Naive;
class Kernel_Transpose_V1;
class Kernel_Transpose_V2;

// ----------------------------------------------------------------

void init_pm(class PM * pm)
{
  pm_ = pm;
}

// gridDim.{x,y,z} == get_num_group({0,1,2})
// blockDim.{x,y,z} == get_local_range({0,1,2})
// blockIdx.{x,y,z} == get_group({0,1,2})
// threadIdx.{x,y,z} == get_local_id({0,1,2})

void copy_naive_gpu(real_t * out, real_t * in, const int num_rows, const int num_cols)
{
  sycl::queue * Q = pm_->dev_get_queue();
  
  sycl::range<3> grid_size(num_rows, _TRANSPOSE_BLOCK_SIZE, 1);
  sycl::range<3> block_size(1, _TRANSPOSE_BLOCK_SIZE, 1);

  sycl::nd_range<3> kernel_rng(grid_size, block_size);
  
  Q->submit([&](sycl::handler &cgh) {

    cgh.parallel_for<Kernel_Copy_Naive>(kernel_rng, [=](sycl::nd_item<3> idx) {
      const int i = idx.get_group(0) * idx.get_local_range(0) + idx.get_local_id(0);

      if(i >= num_rows) return;

      int j = idx.get_group(1) * idx.get_local_range(1) + idx.get_local_id(1);
      
      while(j < num_cols) {
	out[i*num_cols + j] = in[i*num_cols + j];
	j += idx.get_local_range(1);
      }
    }); // End of the kernel function
    
    //  }).wait();       // End of the queue commands
  });       // End of the queue commands
}

// ----------------------------------------------------------------

void transpose_naive_gpu(real_t * out, real_t * in, const int num_rows, const int num_cols)
{
  sycl::queue * Q = pm_->dev_get_queue();
  
  sycl::range<3> grid_size(num_rows, _TRANSPOSE_BLOCK_SIZE, 1);
  sycl::range<3> block_size(1, _TRANSPOSE_BLOCK_SIZE, 1);

  sycl::nd_range<3> kernel_rng(grid_size, block_size);
  
  Q->submit([&](sycl::handler &cgh) {

    cgh.parallel_for<Kernel_Transpose_Naive>(kernel_rng, [=](sycl::nd_item<3> idx) {
      const int i = idx.get_group(0) * idx.get_local_range(0) + idx.get_local_id(0);

      if(i >= num_rows) return;

      int j = idx.get_group(1) * idx.get_local_range(1) + idx.get_local_id(1);
      
      while(j < num_cols) {
	out[j*num_rows + i] = in[i*num_cols + j];
	j += idx.get_local_range(1);
      }
    }); // End of the kernel function
    
    //}).wait();       // End of the queue commands
  });       // End of the queue commands
}

// ----------------------------------------------------------------

void transpose_gpu_v1(real_t * out, real_t * in, const int num_rows, const int num_cols)
{
  sycl::queue * Q = pm_->dev_get_queue();
  
  sycl::range<3> grid_size(_TILE(num_rows, _TRANSPOSE_BLOCK_SIZE), _TILE(num_cols, _TRANSPOSE_NUM_ROWS), 1);
  sycl::range<3> block_size(_TRANSPOSE_BLOCK_SIZE, _TRANSPOSE_NUM_ROWS, 1);

  sycl::nd_range<3> kernel_rng(grid_size, block_size);
  
  Q->submit([&](sycl::handler &cgh) {

    sycl::local_accessor<real_t, 2> cache(sycl::range(_TRANSPOSE_BLOCK_SIZE, _TRANSPOSE_BLOCK_SIZE), cgh);
      
    cgh.parallel_for<Kernel_Transpose_V1>(kernel_rng, [=](sycl::nd_item<3> idx) {
      int irow = idx.get_group(0) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(0);
      int icol = idx.get_group(1) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(1);

      // load tile into fast local memory

      const int indxi = irow * num_cols + icol;
      for(int i=0; i<_TRANSPOSE_BLOCK_SIZE; i+= _TRANSPOSE_NUM_ROWS) {
	if(irow < num_rows && (icol+i) < num_cols) // nonsquare
	  cache[idx.get_local_id(1)+i][idx.get_local_id(0)] = in[indxi + i]; // threads read chunk of a row and write as a column
      }

      // block to ensure reads finish

      idx.barrier(sycl::access::fence_space::local_space);

      // swap indices

      irow = idx.get_group(1) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(0);
      icol = idx.get_group(0) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(1);      

      // write tile to global memory

      const int indxo = irow * num_rows + icol;
      for(int i=0; i<_TRANSPOSE_BLOCK_SIZE; i+= _TRANSPOSE_NUM_ROWS) {
	if(irow < num_cols && (icol+i) < num_rows) // nonsquare
	  out[indxo + i] = cache[idx.get_local_id(0)][idx.get_local_id(1) + i];
      }
    }); // End of the kernel function
    
    //  }).wait();       // End of the queue commands
  });       // End of the queue commands
}

// ----------------------------------------------------------------

void transpose_gpu_v2(real_t * out, real_t * in, const int num_rows, const int num_cols)
{
  sycl::queue * Q = pm_->dev_get_queue();
  
  sycl::range<3> grid_size(_TILE(num_rows, _TRANSPOSE_BLOCK_SIZE), _TILE(num_cols, _TRANSPOSE_NUM_ROWS), 1);
  sycl::range<3> block_size(_TRANSPOSE_BLOCK_SIZE, _TRANSPOSE_NUM_ROWS, 1);

  sycl::nd_range<3> kernel_rng(grid_size, block_size);
  
  Q->submit([&](sycl::handler &cgh) {

    sycl::local_accessor<real_t, 2> cache(sycl::range(_TRANSPOSE_BLOCK_SIZE, _TRANSPOSE_BLOCK_SIZE+1), cgh);
      
    cgh.parallel_for<Kernel_Transpose_V2>(kernel_rng, [=](sycl::nd_item<3> idx) {
      
      int irow = idx.get_group(0) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(0);
      int icol = idx.get_group(1) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(1);

      // load tile into fast local memory

      const int indxi = irow * num_cols + icol;
      for(int i=0; i<_TRANSPOSE_BLOCK_SIZE; i+= _TRANSPOSE_NUM_ROWS) {
	if(irow < num_rows && (icol+i) < num_cols) // nonsquare
	  cache[idx.get_local_id(1)+i][idx.get_local_id(0)] = in[indxi + i]; // threads read chunk of a row and write as a column
      }

      // block to ensure reads finish

      idx.barrier(sycl::access::fence_space::local_space);

      // swap indices

      irow = idx.get_group(1) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(0);
      icol = idx.get_group(0) * _TRANSPOSE_BLOCK_SIZE + idx.get_local_id(1);      

      // write tile to global memory

      const int indxo = irow * num_rows + icol;
      for(int i=0; i<_TRANSPOSE_BLOCK_SIZE; i+= _TRANSPOSE_NUM_ROWS) {
	if(irow < num_cols && (icol+i) < num_rows) // nonsquare
	  out[indxo + i] = cache[idx.get_local_id(0)][idx.get_local_id(1) + i];
      }
    }); // End of the kernel function
    
    //  }).wait();       // End of the queue commands
  });       // End of the queue commands
}

#endif
